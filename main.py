from scrapping import scrape_google_news
import json
import os
import time

with open("keywords.json", "r", encoding="utf-8") as f:
    keywords_json = json.load(f)

keywords = list(set([kw for category in keywords_json.values() for kw in category]))

all_news = []
failed_urls_all = []
output_path = "news_data.json"

if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        all_news = json.load(f)

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36'
}

for i, keyword in enumerate(keywords, 1):
    print(f"\nğŸ” ({i}/{len(keywords)}) AranÄ±yor: {keyword}")
    try:
        results, failed_urls = scrape_google_news(keyword, lang="en", max_results=5, headers=headers)
        all_news.extend(results)
        failed_urls_all.extend(failed_urls)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(all_news, f, ensure_ascii=False, indent=2)
        time.sleep(2)  
    except Exception as e:
        print(f"âš ï¸ {keyword} aranÄ±rken hata oluÅŸtu: {e}")
        continue

with open("failed_urls.json", "w", encoding="utf-8") as f:
    json.dump(list(set(failed_urls_all)), f, ensure_ascii=False, indent=2)

print(f"\nâœ… Toplam haber sayÄ±sÄ±: {len(all_news)} kayÄ±t '{output_path}' dosyasÄ±na kaydedildi.")
print(f"[!] Toplam baÅŸarÄ±sÄ±z URL sayÄ±sÄ±: {len(set(failed_urls_all))}")